{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取政策链接\n",
    "---\n",
    "第一步（cell1：爬取政策链接）\n",
    "在```url = \"https://www.gov.cn/search/zhengce/?q=%E6%96%B0%E8%83%BD%E6%BA%90\"```中爬取\n",
    "其中```q = xxx```为关键词，直接搜索复制url即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻页结束：下一页按钮不可点击\n",
      "链接保存完成，共抓取到 11 个链接\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# 配置 Edge 无头模式\n",
    "edge_options = Options()\n",
    "edge_options.add_argument(\"--headless\")\n",
    "\n",
    "# 实例化 Edge 浏览器\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "url = \"https://sousuo.www.gov.cn/sousuo/search.shtml?code=17da70961a7&searchWord=%E6%99%BA%E8%83%BD%20%E4%BA%A4%E9%80%9A&dataTypeId=14&sign=c6db081f-a631-4440-f051-32774416dae9\"\n",
    "driver.get(url)\n",
    "\n",
    "all_links = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # 等待页面中至少出现一个 a 标签\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "    except Exception as e:\n",
    "        print(\"页面加载失败：\", e)\n",
    "        break\n",
    "\n",
    "    # 获取当前页面渲染后的 HTML，并用 BS4 解析\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # 查找所有 class 为 \"title log-anchor\" 的 a 标签\n",
    "    a_tags = soup.find_all(\"a\", class_=\"title log-anchor\")\n",
    "    for a_tag in a_tags:\n",
    "        href = a_tag.get(\"href\")\n",
    "        if href:\n",
    "            all_links.append(href)\n",
    "    \n",
    "    try:\n",
    "        # 定位下一页按钮（通过 class=\"next\"）\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, \".oPager a.next\")\n",
    "        # 此处可以判断下一页按钮是否禁用（例如检查是否有 disable/lose 标识）\n",
    "        if \"lose\" in next_button.get_attribute(\"class\"):\n",
    "            print(\"翻页结束：下一页按钮不可点击\")\n",
    "            break\n",
    "\n",
    "        next_button.click()\n",
    "        # 等待页面更新\n",
    "        time.sleep(3)\n",
    "    except Exception as e:\n",
    "        print(\"翻页结束或下一页按钮不可点击：\", e)\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "with open(\"links.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for link in all_links:\n",
    "        f.write(link + \"\\n\")\n",
    "\n",
    "print(\"链接保存完成，共抓取到\", len(all_links), \"个链接\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_urls.py\n",
    "\n",
    "# 输入和输出文件路径（根据实际情况修改）\n",
    "input_file = 'links.txt'    # 原始文件\n",
    "output_file = 'urls_reversed.txt'  # 输出文件\n",
    "\n",
    "# 读取原始文件内容\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    # 读取所有行并去除空行\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# 反转列表顺序\n",
    "lines.reverse()\n",
    "\n",
    "# 将倒序后的内容写入新文件\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for line in lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "print(f\"处理完成！已保存倒序后的文件至: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/policy_spider/2016_实现旅客便捷出行、货物高效运输——国家发改委基础产业司副司长解读智能交通发展实施方案.txt 保存成功. 这是成功保存的第 1 个文件.\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2018-12/31/content_5440057.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2018-12/31/content_5442955.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2019-10/15/content_5440071.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2019-11/19/content_5456289.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2019-11/21/content_5454148.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2020-12/28/content_5574286.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2021-03/20/content_5594095.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2021-11/10/content_5650034.htm\n",
      "无法获取有效内容：http://www.gov.cn/zhengce/zhengceku/2022-09/25/content_5711801.htm\n",
      "无法获取有效内容：https://www.gov.cn/zhengce/zhengceku/202404/content_6947020.htm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 从文件中读取所有链接\n",
    "with open(\"urls_reversed.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_links = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# 配置 Edge 的无头模式\n",
    "edge_options = Options()\n",
    "edge_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# 创建保存txt文件的目录\n",
    "output_dir = \"../data/policy_spider/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 定义成功保存的文件计数器\n",
    "saved_count = 0\n",
    "\n",
    "for idx, link in enumerate(all_links):\n",
    "    driver.get(link)\n",
    "    # 预留等待时间，确保动态加载完成\n",
    "    time.sleep(3)\n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "    \n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    year = \"\"\n",
    "    \n",
    "    # 动态遍历标题的候选选择器\n",
    "    title_candidates = [\n",
    "        \"div.mhide.pctoubukuang1 td[colspan='3']\",\n",
    "        \"div.article.oneColumn h1#ti\"\n",
    "    ]\n",
    "    for sel in title_candidates:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag:\n",
    "            title = tag.get_text(strip=True)\n",
    "            if title:\n",
    "                break\n",
    "\n",
    "    # 动态遍历日期（年份）的候选逻辑：\n",
    "    # ① 尝试从包含 “成文日期” 的 td 后面获取\n",
    "    date_found = False\n",
    "    for td in soup.select(\"div.mhide.pctoubukuang1 td.lf15\"):\n",
    "        if \"成文日期\" in td.get_text():\n",
    "            next_td = td.find_next_sibling(\"td\")\n",
    "            if next_td:\n",
    "                time_text = next_td.get_text(strip=True)\n",
    "                m = re.search(r'(\\d{4})', time_text)\n",
    "                if m:\n",
    "                    year = m.group(1)\n",
    "                    date_found = True\n",
    "                    break\n",
    "    # ② 尝试通用日期区域\n",
    "    if not date_found:\n",
    "        date_tag = soup.select_one(\"div.pages-date\")\n",
    "        if date_tag:\n",
    "            time_text = date_tag.get_text(strip=True)\n",
    "            m = re.search(r'(\\d{4})', time_text)\n",
    "            if m:\n",
    "                year = m.group(1)\n",
    "    \n",
    "    # 动态遍历正文的候选选择器\n",
    "    # 每个候选为 (selector, 提取方式)\n",
    "    # 提取方式： \"full\" 为直接全文提取； \"indent\" 为仅提取 p 标签中包含 \"text-indent: 2em\" 的文本\n",
    "    content_candidates = [\n",
    "        (\"table.border-table.noneBorder.pages_content div.b12c.pages_content#UCAP-CONTENT\", \"full\"),\n",
    "        (\"div.trs_editor_view.TRS_UEDITOR.trs_paper_default.trs_web\", \"full\"),\n",
    "        (\"div.view.TRS_UEDITOR.trs_paper_default.trs_web\", \"indent\"),\n",
    "        (\"div.trs_editor_view.TRS_UEDITOR.trs_paper_default\", \"full\"),\n",
    "        (\"div.pages_content.mhide\", \"full\"),\n",
    "        (\"div.pages_content\", \"indent\")  # 新增选择器，针对 pages_content 的 div\n",
    "    ]\n",
    "    for sel, method in content_candidates:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag:\n",
    "            if method == \"full\":\n",
    "                content = tag.get_text(separator=\"\\n\", strip=True)\n",
    "            elif method == \"indent\":\n",
    "                lines = []\n",
    "                for p in tag.find_all(\"p\"):\n",
    "                    style = p.get(\"style\", \"\")\n",
    "                    if \"text-indent: 2em\" in style:\n",
    "                        lines.append(p.get_text(strip=True))\n",
    "                content = \"\\n\".join(lines)\n",
    "            # 只要有内容就认为匹配成功\n",
    "            if content:\n",
    "                break\n",
    "\n",
    "    if not title or not content:\n",
    "        print(f\"无法获取有效内容：{link}\")\n",
    "        continue\n",
    "\n",
    "    # 构造安全的文件名，格式为 \"年份_标题.txt\"（若未提取到年份，则仅用标题）\n",
    "    safe_title = re.sub(r\"[\\\\/*?\\\"<>|]\", \"\", title)\n",
    "    if year:\n",
    "        file_name = f\"{year}_{safe_title}.txt\"\n",
    "    else:\n",
    "        file_name = f\"{safe_title}.txt\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(title + \"\\n\\n\")\n",
    "        f.write(content)\n",
    "    \n",
    "    saved_count += 1\n",
    "    print(f\"{file_path} 保存成功. 这是成功保存的第 {saved_count} 个文件.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义 CSV 文件所在的文件夹路径\n",
    "csv_folder = '../data/weibo_raw'\n",
    "# 定义输出的 TXT 文件的文件名\n",
    "output_txt = 'weibo_content.txt'\n",
    "\n",
    "try:\n",
    "    with open(output_txt, 'w', encoding='utf-8') as txt_file:\n",
    "        # 遍历文件夹中的所有文件\n",
    "        for root, dirs, files in os.walk(csv_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    csv_file = os.path.join(root, file)\n",
    "                    try:\n",
    "                        # 读取 CSV 文件\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        # 检查是否存在“微博正文”列\n",
    "                        if '微博正文' in df.columns:\n",
    "                            # 获取“微博正文”列的数据\n",
    "                            weibo_content = df['微博正文']\n",
    "                            for content in weibo_content:\n",
    "                                if pd.notna(content):\n",
    "                                    # 将内容写入 TXT 文件\n",
    "                                    txt_file.write(str(content) + '\\n')\n",
    "                        else:\n",
    "                            print(f\"文件 {csv_file} 中不存在 '微博正文' 列。\")\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"错误: 文件 {csv_file} 未找到。\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"错误: 读取文件 {csv_file} 时发生未知错误: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"错误: 写入文件 {output_txt} 时发生未知错误: {e}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy_analyse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
